{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# FWI with Devito, Dask, buckets, DFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_IGNORE_OUTPUT\n",
    "\n",
    "# Set up inversion parameters.\n",
    "param = {'t0': 0.,\n",
    "         'tn': 1000.,              # Simulation last 1 second (1000 ms)\n",
    "         'f0': 0.010,              # Source peak frequency is 10Hz (0.010 kHz)\n",
    "         'nshots': 5,              # Number of shots to create gradient from\n",
    "         'm_bounds': (0.08, 0.25), # Set the min and max slowness\n",
    "         'shape': (101, 101),      # Number of grid points (nx, nz).\n",
    "         'spacing': (10., 10.),    # Grid spacing in m. The domain size is now 1km by 1km.\n",
    "         'origin': (0, 0),         # Need origin to define relative source and receiver locations.\n",
    "         'nbpml': 40}              # nbpml thickness.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import scipy\n",
    "from scipy import signal, optimize\n",
    "\n",
    "from devito import Grid\n",
    "\n",
    "from distributed import Client, LocalCluster, wait\n",
    "\n",
    "import cloudpickle as pickle\n",
    "\n",
    "# Import acoustic solver, source and receiver modules.\n",
    "from examples.seismic import Model, demo_model\n",
    "from examples.seismic.acoustic import AcousticWaveSolver\n",
    "from examples.seismic import TimeAxis, PointSource, RickerSource, Receiver\n",
    "\n",
    "# Import convenience function for plotting results\n",
    "from examples.seismic import plot_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_from_bucket(filename):\n",
    "    client = storage.Client(project='seg-demo-project-2')\n",
    "    bucket = client.get_bucket('datasets-proxy')\n",
    "    blob = bucket.get_blob(filename)\n",
    "    with open(filename, 'wb') as f:\n",
    "        blob.download_to_file(f)\n",
    "\n",
    "\n",
    "def upload_file_to_bucket(filename):\n",
    "    client = storage.Client(project='seg-demo-project-2')\n",
    "    bucket = client.get_bucket('datasets-proxy')\n",
    "    blob = storage.Blob(filename, bucket)\n",
    "    with open(filename, 'rb') as f:\n",
    "        blob.upload_from_file(f)\n",
    "\n",
    "\n",
    "def from_hdf5(filename, **kwargs):\n",
    "    import h5py\n",
    "\n",
    "    f = h5py.File(filename, 'r')\n",
    "\n",
    "    origin = kwargs.pop('origin', None)\n",
    "    if origin is None:\n",
    "        origin_key = kwargs.pop('origin_key', 'o')\n",
    "        origin = tuple(f[origin_key][()])\n",
    "\n",
    "    spacing = kwargs.pop('spacing', None)\n",
    "    if spacing is None:\n",
    "        spacing_key = kwargs.pop('spacing_key', 'd')\n",
    "        spacing = tuple(f[spacing_key][()])\n",
    "\n",
    "    nbpml = kwargs.pop('nbpml', 20)\n",
    "    datakey = kwargs.pop('datakey', None)\n",
    "    if datakey is None:\n",
    "        raise ValueError(\"Must specify datakey\")\n",
    "\n",
    "    space_order = kwargs.pop('space_order', None)\n",
    "    dtype = kwargs.pop('dtype', None)\n",
    "    data_m = f[datakey][()]\n",
    "    data_vp = np.sqrt(1/data_m).astype(dtype)\n",
    "    data_vp = np.transpose(data_vp, (1, 2, 0))\n",
    "    shape = data_vp.shape\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    return Model(space_order=space_order, vp=data_vp, origin=origin,\n",
    "                 shape=shape, dtype=dtype, spacing=spacing, nbpml=nbpml)\n",
    "\n",
    "def get_initial_model():\n",
    "    filename = 'overthrust_3D_initial_model.h5'\n",
    "\n",
    "    model_file = Path(filename)\n",
    "    if not model_file.is_file():\n",
    "        download_file_from_bucket(filename)\n",
    "\n",
    "    return from_hdf5(filename, nbpml=param['nbpml'], space_order=8,\n",
    "                     datakey='m', dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "filename = \"/tmp/seg-demo-project-2-2fa1703fb1c7.json\"\n",
    "jsonfile = open(filename, 'w')\n",
    "jsonfile.write(r\"\"\"{\n",
    "  \"type\": \"service_account\",\n",
    "  \"project_id\": \"seg-demo-project-2\",\n",
    "  \"private_key_id\": \"2fa1703fb1c7327ca598b6d6ac7a8d7b3501d524\",\n",
    "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQCUcKay5c3FS+Dc\\nDvMoy3hZtHvuBtu3bGkTClsjHZtnfm65CqFuWyxSpYkDtmi7KR1SQY4+WeP/uX5z\\nl1CQhowr1D3R6IdkeJxuu2+xEKscvAW6UukxUiCo8i7Ov3Ve5zusaplnizT9rxUO\\nCmYNvQYyV/4jViy4LqKphIC9UZtOo+5R7osP8LJmBvCvrVm+hSE7Ob/rHKMcvoPq\\n9odEMdRT8n/4tLJ2auF6XnM2uzKgyt/iAEdlzM6VaSDKlSuNJEXTpqEW/vsQUWnL\\n8G0zOxapsrEar63fMz4qyyP5qxiP8GiGYDi9onhJijDKTR6NlFWMqDewXBJCp+sh\\n+fb+IRhXAgMBAAECggEAOsKpSpIrtZlG5vXIDfMVrjUDBgOTAHYa1h24XBtBRGPJ\\nQtjRdJUE46aBqYWQyd4JrGElBMuitL1iMDHLA5sva741xp1M01nnWvI50ZbulY5c\\nmhhuFUcUhKxyGQezV6EjfyonldSGYpHnPMqjAXc9N7qbcLOROkvDumUobUkbuLIT\\neuVMm3Jfp+fHjBO7pUB5FPd6c5vXtTnp+orf5BpSzQSecHCdf6A4xvPdPScnE15j\\nFYSwc41/dG/hkVtn9DwySgOPsjS6ljUUE57Zitmqy0LeLrPx4QqWhbl/TSeYCOK1\\naM3KrIH79XEslTZzWTwMPi924xKWY06HWI3FBN4siQKBgQDNvQPBx9ajKgYqUM0D\\nWj13x3zlNBACIT+uoSBToLT5YiS0vwlwprw7UvC3fAFCOgvena3UtdcJKdiiWU65\\nAFM8WPfR5ekF1Jg1yGQCe1raYFmV/UCiBzDf6PH8po3OyoTgfWvdye3jClfMtD2L\\nHW9BCLy9oU86J6f5sFMp5OF2wwKBgQC4tCur/x6tNwP3s3rep/jGibo8Okczbfcr\\nNm2g5R3CpY0lPRex3hJ8wcY0vOc63k886KIAnQOvRwpZ5lO/BOLkfiDQk1XLcZ/m\\nPSrQH2MOZupQ3nndkGJdx82jPOcuZKi397aoO8h4yzgsWV5iaHvInq8k+TK95c2p\\nCw4XbScG3QKBgF6KA4bxMGULs4eQV5S5y9MVnQOpt81yv9OcAMHM8DxEZ/+SZWEZ\\njRdplmmKFv36tGeYZz9+S5DPZNe9WBpU1uq9KyuNjVV7inH0YlhtnKMKcUAl/qQ1\\nz7SkU0Y4tqMWlpadq4pf9utEXnIXRMx/OxdUT36H+GMNw/dNmfl/TkeHAoGAF/j6\\nehZgquarEykuV1vBxDL4Av0lZJ1vKSKlU+6o0CyghybIvoMuLxcPXKTv9goIisU+\\n0YmPgt5bj5N/ZxmBQVrFc4zL493ZfQ6PUffg6WueGeTmOEWXHsjh/b/X2YOjCk2S\\nXX9044isv8TRpUAeYMmHverCFTeQW9Jdf9jg6dkCgYA5pHBk/XrfZDori354jYIw\\nuJgcrPmvLf72wSNAMdKBLXLoYEcZ1hCp6iLaM6hQwXa8vzLLPzprDPe5ZCqVp3sE\\nQLS0Rpd0UAs/y4XKGKTVq4xgXUTRS0x91GB1Qs3HiI2DN2duJfanUs2eJd4A2FJJ\\nXb9XeDzqftxsEgsK+LpV6A==\\n-----END PRIVATE KEY-----\\n\",\n",
    "  \"client_email\": \"gerardmac@seg-demo-project-2.iam.gserviceaccount.com\",\n",
    "  \"client_id\": \"107431839642113475492\",\n",
    "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/gerardmac%40seg-demo-project-2.iam.gserviceaccount.com\"\n",
    "}\n",
    "\"\"\")\n",
    "jsonfile.close()\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the shots in a bucket.\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "def list_blobs_with_prefix(bucket_name, prefix):\n",
    "    \"\"\"Lists all the blobs in the bucket that begin with the prefix.\n",
    "\n",
    "    This can be used to list all blobs in a \"folder\", e.g. \"public/\".\n",
    "\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client(project='seg-demo-project-2')\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "\n",
    "    return [blob.name for blob in blobs]\n",
    "\n",
    "list_of_shots = list_blobs_with_prefix('datasets-proxy', \"shot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename):\n",
    "    \"\"\" Returns the current model. This is used by the\n",
    "    worker to get the current model.\n",
    "    \"\"\"\n",
    "\n",
    "    model_file = Path(filename)\n",
    "    if not model_file.is_file():\n",
    "        download_file_from_bucket(filename)\n",
    "\n",
    "    pkl = pickle.load(open(filename, \"rb\"))\n",
    "    \n",
    "    return pkl['model']\n",
    "\n",
    "def dump_model(filename, model):\n",
    "    ''' Dump model to disk.\n",
    "    '''\n",
    "    pickle.dump({'model':model}, open(filename, \"wb\"))\n",
    "    \n",
    "    upload_file_to_bucket(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shot_data(filename, dt):\n",
    "    ''' Load shot data from bucket, resampling to the model time step.\n",
    "    '''\n",
    "    model_file = Path(filename)\n",
    "    if not model_file.is_file():\n",
    "        download_file_from_bucket(filename)\n",
    "    \n",
    "    pkl = pickle.load(open(filename, \"rb\"))\n",
    "    \n",
    "    return pkl['src'].resample(dt), pkl['rec'].resample(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a type to store the functional and gradient.\n",
    "class fg_pair:\n",
    "    def __init__(self, f, g):\n",
    "        self.f = f\n",
    "        self.g = g\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        f = self.f + other.f\n",
    "        g = self.g + other.g\n",
    "        \n",
    "        return fg_pair(f, g)\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        if other == 0:\n",
    "            return self\n",
    "        else:\n",
    "            return self.__add__(other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from devito import Function\n",
    "\n",
    "# Create FWI gradient kernel for a single shot\n",
    "def fwi_gradient_i(param):\n",
    "    from devito import clear_cache\n",
    "\n",
    "    # Need to clear the workers cache.\n",
    "    clear_cache()\n",
    "\n",
    "    # Load the current model and the shot data for this worker.\n",
    "    # Note, unlike the serial example the model is not passed in\n",
    "    # as an argument. Broadcasting large datasets is considered\n",
    "    # a programming anti-pattern and at the time of writing it\n",
    "    # it only worked relaiably with Dask master. Therefore, the\n",
    "    # the model is communicated via a file.\n",
    "    model0 = load_model(param['model'])\n",
    "    \n",
    "    dt = model0.critical_dt\n",
    "\n",
    "    src, rec = load_shot_data(param['shot_filename'], dt)\n",
    "    \n",
    "    # Set up solver.\n",
    "    solver = AcousticWaveSolver(model0, src, rec, space_order=8)\n",
    "\n",
    "    # Compute simulated data and full forward wavefield u0\n",
    "    d, ufr, ufi = solver.forward_freq_modeling(freq=(0.003, 0.004, 0.005), factor=10, src=src)\n",
    "\n",
    "    # Compute the data misfit (residual) and objective function\n",
    "    residual = Receiver(name='rec', grid=model0.grid,\n",
    "                        time_range=rec.time_range,\n",
    "                        coordinates=rec.coordinates.data)\n",
    "\n",
    "    residual.data[:] = d.data[:] - rec.data[:]\n",
    "    f = .5*np.linalg.norm(residual.data.flatten())**2\n",
    "    \n",
    "    # Compute gradient using the adjoint-state method. Note, this\n",
    "    # backpropagates the data misfit through the model.\n",
    "    grad = Function(name=\"grad\", grid=model0.grid)\n",
    "\n",
    "    solver.adjoint_freq_born(recin=residual, freq=(0.003, 0.004, 0.005), factor=10, ufr=ufr, ufi=ufi, grad=grad)\n",
    "    \n",
    "    # Copying here to avoid a (probably overzealous) destructor deleting\n",
    "    # the gradient before Dask has had a chance to communicate it.\n",
    "    g = np.array(grad.data[:])\n",
    "    \n",
    "    # return the objective functional and gradient.\n",
    "    return fg_pair(f, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def fwi_gradient(model, param, iid):\n",
    "    # Dump a copy of the current model for the workers\n",
    "    # to pick up when they are ready.\n",
    "    param['model'] = \"model_%d.p\"%iid\n",
    "    dump_model(param['model'], model)\n",
    "\n",
    "    # Select a random sample of shots - choosing 28 because that's\n",
    "    # the size of my test cluster.\n",
    "    minibatch = random.sample(list_of_shots, 28)\n",
    "    \n",
    "    # Create worklist.\n",
    "    worklist = []\n",
    "    for shot in minibatch:\n",
    "        worklist.append(dict(param))\n",
    "        worklist[-1]['shot_filename'] = shot        \n",
    "        \n",
    "    # Distribute worklist.\n",
    "    fgi = client.map(fwi_gradient_i, work, retries=1)\n",
    "    \n",
    "    # Perform reduction.\n",
    "    fg = client.submit(sum, fgi).result()\n",
    "    \n",
    "    return fg.f, fg.g\n",
    "\n",
    "# Define bounding box constraints on the solution.\n",
    "def apply_box_constraint(m):\n",
    "    # Maximum possible 'realistic' velocity is 3.5 km/sec\n",
    "    # Minimum possible 'realistic' velocity is 2 km/sec\n",
    "    return np.clip(m, 1/3.5**2, 1/2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Dask cluster\n",
    "cluster = LocalCluster(n_workers=5, death_timeout=600)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwi_iterations = 5\n",
    "model_i = get_initial_model()\n",
    "\n",
    "# Run FWI with gradient descent\n",
    "history = np.zeros((fwi_iterations, 1))\n",
    "for i in range(fwi_iterations):\n",
    "    # Compute the functional value and gradient for the current\n",
    "    # model estimate\n",
    "    phi, direction = fwi_gradient(model_i, param, i)\n",
    "    \n",
    "    # Store the history of the functional values\n",
    "    history[i] = phi\n",
    "    \n",
    "    # Artificial Step length for gradient descent\n",
    "    # In practice this would be replaced by a Linesearch (Wolfe, ...)\n",
    "    # that would guarantee functional decrease Phi(m-alpha g) <= epsilon Phi(m)\n",
    "    # where epsilon is a minimum decrease constant\n",
    "    alpha = .005 / np.max(direction)\n",
    "    \n",
    "    # Update the model estimate and inforce minimum/maximum values\n",
    "    model_i.m.data[:] = apply_box_constraint(model_i.m.data - alpha * direction)\n",
    "    \n",
    "    # Log the progress made\n",
    "    print('Objective value is %f at iteration %d' % (phi, i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply our FWI function and have a look at the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "\n",
    "# Show what the update does to the model\n",
    "from examples.seismic import plot_image, plot_velocity\n",
    "\n",
    "model0.vp = np.sqrt(1. / model0.m.data[40:-40, 40:-40])\n",
    "plot_velocity(model0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "\n",
    "# Plot percentage error\n",
    "plot_image(100*np.abs(model0.vp-get_true_model().vp.data)/get_true_model().vp.data, vmax=15, cmap=\"hot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot objective function decrease\n",
    "plt.figure()\n",
    "plt.loglog(relative_error)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('True relative error')\n",
    "plt.title('Convergence')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
